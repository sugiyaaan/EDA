---
title: "EDA Assignment 6"
author: "Motoki Sugino"
date: "10/27/2016"
output: html_document
---
#PART A:  I choose the boston marathon data set.

boston.marathon.wtimes  

This data set gives the winning time in the men’s Boston Marathon (a famous race about 26 miles) for the years 1887-1996.              

For your dataset.

###1.  Using R, perform a resistant smooth (3RSSH, twice) to your data.  Save the smooth (the fit) and the rough (the residuals).
```{r}
library(LearnEDA)
head(boston.marathon.wtimes)
with(boston.marathon.wtimes, plot(year, minutes))
smooth.3R <- smooth(boston.marathon.wtimes$minutes, kind="3R")
smooth.3R[1:12]
with(boston.marathon.wtimes,
plot(year, smooth.3R,type="l",col="red",lwd=2, xlab="YEAR",ylab="MINUTES",
main="3R SMOOTH", ylim=c(120,180)))
```

```{r}
#Splitting
smooth.3RSS <- smooth(boston.marathon.wtimes$minutes, kind="3RSS")
with(boston.marathon.wtimes,
plot(year, smooth.3R,type="l",col="blue",lwd=2, xlab="YEAR",ylab="MINUTES",
main="3R and 3RSS Smooths",
ylim=c(120,180)))
with(boston.marathon.wtimes,
lines(year, smooth.3RSS, col="red"))
legend("topleft", legend=c("3R", "3RSS"),
lty=1, col=c("blue", "red"))
```

```{r}
#Hanning
smooth.3RSSH <- han(smooth.3RSS) 
with(boston.marathon.wtimes,
plot(year, smooth.3RSSH, type="l",col="red",lwd=2, 
      xlab="YEAR", ylab="MINUTES", 
      main="3RSSH SMOOTH", ylim=c(120,180)))
```

```{r}
#The smooth and the rough (DATA = SMOOTH + ROUGH)
Rough <- with(boston.marathon.wtimes, minutes - smooth.3RSSH)
with(boston.marathon.wtimes,
head(cbind(minutes, smooth.3RSSH, Rough)))

#Reroughing
options(width=60) 
as.vector(Rough)
smooth.3RS3R.twice <- smooth(boston.marathon.wtimes$minutes, kind="3RS3R", twiceit=TRUE)
```

We have saved the smooth (the fit) and the rough (the residuals).

###2.  Plot the smooth (using a smooth curve) and describe the general patterns that you see.  Don’t assume that anything is obvious – pretend that you are explaining this to someone who doesn’t have any background in statistics.
```{r}
with(boston.marathon.wtimes, plot(year, smooth.3RS3R.twice,
        col="red", lwd=2,  type="l",
        main="3RSSR, Twice Smooth"),ylim=c(120,180))
```

The boston.marathon’s winning time had an immediate drop between **1897** to **1900**.The winning time reached a peak about **1897**.
After this early drop, the time had an immediate drop between **1904** to **1910**.
After those two drops, the time showed a general increase from about year **1911** to about year **1927**.
After this increase, the time showed a gereral decrease from about year 1928 to about year **1952**.
As Dr Albert explained in our classnotes, there is a dip in the graph in the mid-50’s due to the shorter running distance. Therefore, the time had an immediate drop in mid-50's. After this the time showed a increase till about year **1963**. 
From year **1964**, the time showed a general decrease again till about year **1975**. This suggests athletes are improving due to better equipment, better training, etc.
The times since **1980** have stayed pretty constant, suggesting that maybe there is a leveling off in performance in this race.

###3.  Plot the rough (as a time series plotting individual points).  Do you see any general patterns in the rough?
```{r}
FinalRough <- boston.marathon.wtimes$minutes - smooth.3RS3R.twice 
plot(boston.marathon.wtimes$year, FinalRough,
pch=19, cex=1.3, xlab="YEAR", ylab="ROUGH") 
abline(h=0, lwd=3, col="blue")
with(boston.marathon.wtimes, plot(year, FinalRough,
        col="red", lwd=2,  type="l",
        main="FinalRough"))
```

The highest rought is in about year **1910** and the lowest is in about year **1909**.
There are **5** times of big increase and decrease in the plot.
The deviations are constant between year **1912** and **1925** and between year **1935** to year **1965**. 
After year **1965**, the deviations are getting smaller. 
From year **1910** to year **1996**, looking at the entire plot, there are a decreasing trend as a time series plotting individual points.

###4.  Construct a stemplot and letter value display of the sizes of the rough.  (The size is the absolute value of the rough.)   Set up fences and look for outliers.  Summarize what you have learned.  (What is a typical size of a residual?  Are there any unusually large residuals?)
```{r}
absrough <- abs(FinalRough)
aplpack::stem.leaf(absrough, depth=FALSE)

fivenum(absrough)
dF=4-0;dF
STEP=1.5*dF;STEP
FenceL=0-STEP;FenceL
FenceU=4+STEP;FenceU # x > FenceU should be outlier
FENCEL=0-(2*STEP);FENCEL
FENCEU=4+(2*STEP);FENCEU
```

Summarizing the size of the rough.
The distance between the lower and upper fourths is **4** which is the width of the middle 50% of the data. Therefore we can say that a typical size of a residual is from **0** to **4**.
The upper fence is **10**, so there are **6** unusual large residuals $(11,12,12,14,14,31)$.



#PART B:  Find some data collected over time (with at least 50 values) that would benefit with a smooth.   Plot the smooth and describe the basic patterns that you see.  Plot the rough and look for general patterns and any unusual values.

data source: https://fred.stlouisfed.org/series/JPNNGDP
I am going to use this data about "Gross Domestic Product for Japan".

```{r}

data <- read.csv("https://docs.google.com/spreadsheets/d/1eawH-3Xpfj1bEjlyLnXHgJ77LFUhMopUwBuaG3HTvYU/pub?output=csv")
#data <- read.csv("https://docs.google.com/spreadsheets/d/1XpEDekmOCt8v_wEIZcEnvNtK3AaCiDBNvJ6NFYSBlPQ/pub?output=csv")

head(data)
with(data, plot(observation_date, JPNNGDP))
smooth.3R <- smooth(data$JPNNGDP, kind="3R")
smooth.3R[1:12]
with(data,
plot(observation_date, smooth.3R,type="l",col="red",lwd=2, xlab="observation_date",ylab="JPNNGDP",
main="3R SMOOTH"))
with(data,lines(observation_date, smooth.3R, col="red"))
```



```{r}
smooth.3RSS <- smooth(data$JPNNGDP, kind="3RSS")
with(data,
plot(observation_date, smooth.3R,type="l",col="blue",lwd=2, xlab="observation_date",ylab="GDP",
main="3R and 3RSS Smooths"))
with(data,lines(observation_date, smooth.3R, col="blue"))
with(data,lines(observation_date, smooth.3RSS, col="red"))
legend("topright", legend=c("3R", "3RSS"),
lty=1, col=c("blue", "red"))
```

We are somewhat successful in removing these two-wide peaks and valleys.

```{r}
#Hanning
smooth.3RSSH <- han(smooth.3RSS) 
with(data,
plot(observation_date, smooth.3RSSH, type="l",col="red",lwd=2, 
      xlab="observation_date", ylab="GDP", 
      main="3RSSH SMOOTH"))
with(data,lines(observation_date, smooth.3RSSH, col="red"))
```

This hanning operation is good in smoothing the bumpy monotone sequences that we saw in the 3RSS smooth.

```{r}
#The smooth and the rough (DATA = SMOOTH + ROUGH)
Rough <- with(data, JPNNGDP - smooth.3RSSH)
with(data,
head(cbind(JPNNGDP, smooth.3RSSH, Rough)))
```


```{r}
#Reroughing
options(width=60) 
as.vector(Rough)
smooth.3RS3R.twice <- smooth(data$JPNNGDP, kind="3RS3R", twiceit=TRUE)

with(data, plot(observation_date, smooth.3RS3R.twice,
        col="red", lwd=2,  type="l",
        main="3RSSR, Twice Smooth"))
with(data,lines(observation_date, smooth.3RS3R.twice, col="red"))
```

Japanese GDP had an immediate increase between year **1994** to **1997** and GDP in Japan reached a peak about **1997**.
After this early increase, the Japanese GDP had an immediate drop between year **1997** to **1998**.
The year between **1998** and **2000** have stayed pretty constant,suggesting that maybe Japanese economy was stable at that time .
There is an immediate drop between year **2001** to year **2003**.
Then,the year between **2003** and **2004** have a general increase.
GDP for Japan had an immediate drop between year **2008** to **2009**. This makes sense because bankruptcy of lehman brothers damaged world's economy and effects japanese economy as well.
There is small bump between year **2009** and year **2011** and there is an immediate increase between year **2011** to year **2016**.
Current Japanese GDP is getting closer to the GDP value in **2008**.

```{r}
FinalRough <- data$JPNNGDP - smooth.3RS3R.twice 
plot(data$observation_date, FinalRough,
pch=19, cex=1.3, xlab="YEAR", ylab="ROUGH") 
abline(h=0, lwd=3, col="blue")
with(data, plot(observation_date, FinalRough,
        col="red", lwd=2,  type="l",
        main="FinalRough"))
with(data,lines(observation_date, FinalRough, col="red"))
```

There are two huge negative deviation in **2011** which are **-8483.4** and **-10368.0**. 
Except those two deviation,overall the rough is pretty constant betwenn 5000 to **-5000**.
while there are a number of years (6 to be exact) where the rough is around **+5000**, there are a number of years (9 to be exact) where the rough is around **-5000**.
This might indicates that neggative impact on Japanese economy is very influential.
This is very interesting that the rough in **2008** where the collapse of lehman brothers occurred is not unusual.

```{r}
absrough <- abs(FinalRough)
aplpack::stem.leaf(absrough, depth=FALSE)

fivenum(1)
dF=1326.8-0;dF
STEP=1.5*dF;STEP
FenceL=0-STEP;FenceL
FenceU=1326.8+STEP;FenceU # x > FenceU should be outlier
FENCEL=0-(2*STEP);FENCEL
FENCEU=1326.8+(2*STEP);FENCEU
```

Summarizing the size of the rough.
The distance between the lower and upper fourths is **1326.8** which is the width of the middle 50% of the data. Therefore we can say that a typical size of a residual is from **0** to **1326.8**.
The upper FENCE is **5307.2** so there are **4** unusual large residuals $(5937.8, 6105.8, 8483.4, 10368.0)$.


